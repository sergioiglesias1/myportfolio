<!DOCTYPE html>
<html lang="en">
<head>
	<title>SVM Optimization</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
 	<link rel="stylesheet" href="assets/css/main.css" />
  	<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
 	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">
</head>

<body class="is-preload">
	<div id="wrapper" class="fade-in">		    
		<header id="header"></header>
		  <nav id="nav">
			<ul class="links">
				<li><a href="index.html">Main</a></li>
			</ul>
			<ul class="icons">
				<li><a href="https://www.linkedin.com/in/sergio-iglesias-179aa323b/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
				<li><a href="https://github.com/sergioiglesias" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
			</ul>
		</nav>
  </div>
</body>
	
<div id="main">
    <article class="post">
        <header class="major">
            <h1>Iris Flower Classification: A Machine Learning Project with SVM Optimization in Python</h1>
            <p>5-minute read</p>
        </header>
        <div class="content">
            <p>In this project we use the famous <a href="https://archive.ics.uci.edu/ml/datasets/iris">Iris dataset</a>, one of the most widely used datasets in machine learning. 
            It contains 150 flower samples, equally distributed among three species: <em>setosa</em>, <em>versicolor</em>, and <em>virginica</em>. Each flower is described by four numerical features:</p>
            <ul>
                <li><strong>Sepal length (cm)</strong> — The length of the sepal.</li>
                <li><strong>Sepal width (cm)</strong> — The width of the sepal.</li>
                <li><strong>Petal length (cm)</strong> — The length of the petal.</li>
                <li><strong>Petal width (cm)</strong> — The width of the petal.</li>
            </ul>
            <p>The goal is to build a classification model that predicts the flower species based on these features. 
            To achieve this, we use a Support Vector Machine (SVM) classifier with a pipeline that includes:</p>
            <ul>
                <li><strong>Data preprocessing</strong> — Features are standardized using <code>StandardScaler</code>.</li>
                <li><strong>Model selection</strong> — An SVM with an RBF kernel is trained.</li>
                <li><strong>Hyperparameter tuning</strong> — The parameters <code>C</code> and <code>gamma</code> are optimized using <code>GridSearchCV</code> with cross-validation.</li>
                <li><strong>Performance tracking</strong> — Performance is measured with accuracy, precision, recall, F1-score, and confusion matrices.</li>
            </ul>

<h2>Import packages</h2>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, 
                           ConfusionMatrixDisplay, precision_score, recall_score, f1_score)
from sklearn.pipeline import Pipeline</code></pre>

<h2>Load dataset and Create dataframe</h2>
<pre><code>data = load_iris()
X, y = data.data, data.target
feature_names = data.feature_names
target_names = data.target_names

df = pd.DataFrame(X, columns=feature_names)
df['target'] = y
df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

print(f"Dataset Dimensions: {X.shape}")
print(f"Available Characteristics: {feature_names}")
</code></pre>
<li>Dataset Dimensions: (150, 4)</li>
<li>Available Characteristics: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']</li>
<h2>Train-Test</h2>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training Variable: {X_train.shape[0]} samples")
print(f"Testing Variable: {X_test.shape[0]} samples")
</code></pre>
As the test size is 0.2 (20%) and the number of samples is 150:
<li>Training Variable: 120 samples</li>
<li>Testing Variable: 30 samples</li>
<h2>Pipeline for SVM</h2>
<pre><code>svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='rbf', random_state=42, probability=True))
])</code></pre>

<h2>Optimal hyperparameter search</h2>
<pre><code>svm_param_grid = {
    'svm__C': [0.1, 1, 10, 100],
    'svm__gamma': [0.001, 0.01, 0.1, 1, 'scale']
}</code></pre>
<strong>Hyperparameters:</strong>
    <li>C (Regularization Strength):
      Controls the trade-off between maximizing the margin and minimizing classification errors.
      Tested values: [0.1, 1, 10, 100]</li>
   <li>γ (Gamma - Kernel Coefficient):
      Defines the reach of each training example's influence in the RBF kernel.
      Tested values: [0.001, 0.01, 0.1, 1] with the purpose of scaling the kernel's influence proportionally</li>
	<li>These two formulas will be used: RBF Kernel (Radial Basis Function) and SVM Loss Function with L2 Regularization</li>
	<img src="images/RBFKernel.png" style="max-width:100%; height:auto;">
	<img src="images/RBFKernelLoss.png" style="max-width:100%; height:auto;">
   <p>RBF Kernel</p>
    <ul>
        <li><strong>x<sub>i</sub>, x<sub>j</sub></strong>: data vectors (training samples).</li>
        <li><strong>‖x<sub>i</sub> - x<sub>j</sub>‖<sup>2</sup></strong>: squared Euclidean distance between the two points.</li>
        <li><strong>σ</strong>: controls the width of the Gaussian.</li>
        <li><strong>k(x<sub>i</sub>, x<sub>j</sub>)</strong>: similarity measure (1 = similar, 0 = far apart)</li>
    </ul>
    <p>In SVM Loss with L2 Regularization:</p>
    <ul>
        <li><strong>w</strong>: weight vector of the hyperplane.</li>
        <li><strong>b</strong>: bias term.</li>
        <li><strong>‖w‖<sup>2</sup></strong>: L2 regularization term (penalizes large weights).</li>
        <li><strong>C</strong>: trade-off parameter between margin size and correct classification.</li>
        <li><strong>n</strong>: number of samples.</li>
        <li><strong>y<sub>i</sub></strong>: label of sample i (1 or -1).</li>
        <li><strong>φ(x<sub>i</sub>)</strong>: feature transformation of x<sub>i</sub> (done by the RBF kernel).</li>
        <li><strong>max(0, 1 - y<sub>i</sub>(…))</strong>: hinge loss (penalty if a sample is misclassified or too close to the margin).</li>
    </ul>
<h2>Best SVM model</h2>
<pre><code>svm_grid = GridSearchCV(svm_pipeline, svm_param_grid, cv=5)
svm_grid.fit(X_train, y_train)
best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(X_test)
y_pred_proba_svm = best_svm.predict_proba(X_test)</code></pre>
This code tunes an SVM model via grid search with 5-fold CV, fits it to training data, gets the best estimator, and predicts labels and probabilities on test data.
<h2>Create subplots and set the figure size</h2>
<pre><code>figura = plt.figure(figsize=(22, 16))
colors = {'setosa':'red', 'versicolor':'green', 'virginica':'blue'}
ax2 = figura.add_subplot(111, projection='3d')
for specie, df in df.groupby('species'):
	ax2.scatter(df['sepal width (cm)'], df['petal width (cm)'], df['petal length (cm)'], c=colors[specie], label=specie, s=40)
ax2.set_xlabel("sepal width")
ax2.set_ylabel("petal width")
ax2.set_zlabel("petal length")
ax2.set_title("Iris Scatter 3D", fontweight='bold')
ax2.legend()

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_svm)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(ax=axes[0,0], cmap='Blues')
axes[0,0].set_title('CONFUSION MATRIX', fontweight='bold')

# Correlation matrix located in axes[0,1]
corr_matrix = df[['sepal length (cm)', 'sepal width (cm)', 
                  'petal length (cm)', 'petal width (cm)']].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
            ax=axes[0,1], fmt='.2f', square=True)
axes[0,1].set_title('CORRELATION MATRIX', fontweight='bold')

# Petal length distribution located in axes[1,0]
sns.boxplot(x='species', y='petal length (cm)', data=df, ax=axes[1,0], 
            hue='species',
            palette=["#E61313", "#0CCF60", "#170DDF"],
            legend=False)
axes[1,0].set_title('PETAL LENGTH DISTRIBUTION BY SPECIES', fontweight='bold')
axes[1,0].set_ylabel('Petal Length (cm)')
axes[1,0].set_xlabel('Specie')

# Sepal width distribution located in axes[1,1]
sns.histplot(data=df, x='sepal width (cm)', hue='species', 
             ax=axes[1,1], palette=['#E61313', '#0CCF60', '#170DDF'], 
             alpha=0.6, element='step', kde=True)
axes[1,1].set_title('SEPAL WIDTH DISTRIBUTION BY SPECIES', fontweight='bold')
axes[1,1].set_xlabel('Sepal Width (cm)')

# Last step of creating subplots
plt.tight_layout()
plt.show()</code></pre>

<h2>Accuracy results</h2>
<pre><code>print(f"Cross-validation accuracy: {svm_grid.best_score_:.5f}")
print(f"Test set accuracy: {accuracy_score(y_test, y_pred_svm):.5f}")
</code></pre>
Regarding accuracy results we have:
<li><strong>Cross-validation accuracy = </strong>0.97500</li>
<li><strong>Test set accuracy = </strong>0.96667</li>
<h2>Metrics by class</h2>
<pre><code>precision = precision_score(y_test, y_pred_svm, average=None)
recall = recall_score(y_test, y_pred_svm, average=None)
f1 = f1_score(y_test, y_pred_svm, average=None)

report = classification_report(y_test, y_pred_svm, target_names=target_names, digits=5)
print(f"{report}")
</code></pre>
<img src="images/Captura de pantalla 2025-08-29 114925.png" style="max-width:100%; height:auto;">
    <p>The empty cells in the "accuracy" row (under precision and recall) exist because accuracy is a single overall metric for the model, not broken down by those categories.</p>
	<h2>Final Output</h2>
	<pre><code>results_df = pd.DataFrame({
    'Real': y_test,
    'Prediction': y_pred_svm,
    'Correct': y_test == y_pred_svm
})

results_df['Real_Especie'] = results_df['Real'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})
results_df['Pred_Especie'] = results_df['Prediction'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

print(f"NUMBER OF CORRECT PREDICTIONS: {results_df['Correct'].sum()}/{len(results_df)}")
print(f"FINAL ACCURACY: {accuracy_score(y_test, y_pred_svm):.5f}")
print(f"BEST HYPERPARAMETERS: {svm_grid.best_params_}")
</code></pre>
<p>We finally have:</p>
<ul class="results-list">
  <li><strong>Number of correct predictions:</strong> 29/30</li>
  <li><strong>Final accuracy:</strong> 96.667% (0.96667)</li>
  <li><strong>Best hyperparameters:</strong> 
    <ul>
      <li><strong>C:</strong> 1</li>
      <li><strong>γ:</strong> 0.1</li>	
	<img src="images/output_scatterplot.png" style="max-width:100%; height:auto;">
	<img src="images/cap1.png" style="max-width:100%; height:auto;">
	<img src="images/cap2.png" style="max-width:100%; height:auto;">
	<img src="images/cap3.png" style="max-width:100%; height:auto;">
	<img src="images/cap4 (3).png" style="max-width:100%; height:auto;">
	</ul>
  </li>
</ul>
			<h2>Conclusions</h2>
                <ul>
                    <li>Petal features are more discriminative than sepal features</li>
                    <li>The SVM model achieves high accuracy in multiclass classification</li>
                    <li>The species <em>setosa</em> is the easiest to classify, however, <em>versicolor</em> and <em>virginica</em> show some overlap</li>
                </ul>
    <p>This project demonstrates how machine learning can extract meaningful patterns, in this specific case, from botanical data. Could we apply this same approach to your classification challenge? Let's connect to discuss how SVM or other ML models could solve your business problems.</p>				
		<a href="https://github.com/sergioiglesias1/ML-Clasification-of-Iris-SVM" class="button">View Full Code on GitHub</a>
				</div>
			</article>
		</div>
	</div>

	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
</body>

</html>




















