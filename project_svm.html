 <!DOCTYPE html>
<html lang="en">
<head>
	<title>SVM Optimization</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>

<body class="is-preload">
  <div id="wrapper" class="fade-in">
	<header id="header"></header>
	
<body class="is-preload">
	<div id="wrapper" class="fade-in">		    
		<header id="header"></header>
		  <nav id="nav">
			<ul class="links">
				<li><a href="index.html">Opening</a></li>
				<li class="active"><a href="project.html">Projects</a></li>
			</ul>
			<ul class="icons">
				<li><a href="https://www.linkedin.com/in/sergio-iglesias-179aa323b/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
				<li><a href="https://github.com/sergioiglesias" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
			</ul>
		</nav>
  </div>
</body>
	
<div id="main">
    <article class="post">
        <header class="major">
            <h1>Iris Flower Classification: A Machine Learning Project with SVM Optimization in Python</h1>
            <p>5-minute read</p>
        </header>
        <div class="content">
            <p>In this project we use the famous <a href="https://archive.ics.uci.edu/ml/datasets/iris">Iris dataset</a>, one of the most widely used datasets in machine learning. 
            It contains 150 flower samples, equally distributed among three species: <em>setosa</em>, <em>versicolor</em>, and <em>virginica</em>. Each flower is described by four numerical features:</p>
            <ul>
                <li><strong>Sepal length (cm)</strong> — The length of the sepal.</li>
                <li><strong>Sepal width (cm)</strong> — The width of the sepal.</li>
                <li><strong>Petal length (cm)</strong> — The length of the petal.</li>
                <li><strong>Petal width (cm)</strong> — The width of the petal.</li>
            </ul>
            <p>The goal is to build a classification model that predicts the flower species based on these features. 
            To achieve this, we use a Support Vector Machine (SVM) classifier with a pipeline that includes:</p>
            <ul>
                <li><strong>Data preprocessing</strong> — Features are standardized using <code>StandardScaler</code>.</li>
                <li><strong>Model selection</strong> — An SVM with an RBF kernel is trained.</li>
                <li><strong>Hyperparameter tuning</strong> — The parameters <code>C</code> and <code>gamma</code> are optimized using <code>GridSearchCV</code> with cross-validation.</li>
                <li><strong>Performance tracking</strong> — Performance is measured with accuracy, precision, recall, F1-score, and confusion matrices.</li>
            </ul>

<h2>Import packages</h2>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, 
                           ConfusionMatrixDisplay, precision_score, recall_score, f1_score)
from sklearn.pipeline import Pipeline</code></pre>

<h2>Load dataset and Create dataframe</h2>
<pre><code>data = load_iris()
X, y = data.data, data.target
feature_names = data.feature_names
target_names = data.target_names

df = pd.DataFrame(X, columns=feature_names)
df['target'] = y
df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

print(f"Dataset Dimensions: {X.shape}")
print(f"Available Characteristics: {feature_names}")
</code></pre>
Dataset Dimensions: (150, 4)
Available Characteristics: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
<h2>Train-Test</h2>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training Variable: {X_train.shape[0]} samples")
print(f"Testing Variable: {X_test.shape[0]} samples")
</code></pre>
As the test size is 0.2 (20%) and the number of samples is 150:
Training Variable: 120 samples
Testing Variable: 30 samples
<h2>Pipeline for SVM</h2>
<pre><code>svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='rbf', random_state=42, probability=True))
])</code></pre>

<h2>Optimal hyperparameter search</h2>
<pre><code>svm_param_grid = {
    'svm__C': [0.1, 1, 10, 100],
    'svm__gamma': [0.001, 0.01, 0.1, 1, 'scale']
}</code></pre>
Hyperparameters:
C (Regularization Strength):
  Controls the trade-off between maximizing the margin and minimizing classification errors.  
  Tested values: [0.1, 1, 10, 100]
γ (Gamma - Kernel Coefficient):
  Defines the reach of each training example's influence in the RBF kernel.  
  Tested values: [0.001, 0.01, 0.1, 1] with the purpose of scaling the kernel's influence proportionally
RBF Kernel (Radial Basis Function):
   $$K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\gamma \cdot \|\mathbf{x}_i - \mathbf{x}_j\|^2\right)$$
SVM Loss Function (with L2 Regularization):
   $$\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \max(0, 1 - y_i (\mathbf{w} \cdot \phi(\mathbf{x}_i) + b))$$
   where $\phi(\mathbf{x}_i)$ is the nonlinear transformation applied via the RBF kernel.
<h2>Best SVM model</h2>
<pre><code>svm_grid = GridSearchCV(svm_pipeline, svm_param_grid, cv=5)
svm_grid.fit(X_train, y_train)
best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(X_test)
y_pred_proba_svm = best_svm.predict_proba(X_test)</code></pre>
This code tunes an SVM model via grid search with 5-fold CV, fits it to training data, gets the best estimator, and predicts labels and probabilities on test data.
<h2>Create subplots and set the figure size</h2>
<pre><code>fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_svm)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(ax=axes[0,0], cmap='Blues')
axes[0,0].set_title('CONFUSION MATRIX', fontweight='bold')

# Correlation matrix located in axes[0,1]
corr_matrix = df[['sepal length (cm)', 'sepal width (cm)', 
                  'petal length (cm)', 'petal width (cm)']].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
            ax=axes[0,1], fmt='.2f', square=True)
axes[0,1].set_title('CORRELATION MATRIX', fontweight='bold')

# Petal length distribution located in axes[1,0]
sns.boxplot(x='species', y='petal length (cm)', data=df, ax=axes[1,0], 
            hue='species',
            palette=["#E61313", "#0CCF60", "#170DDF"],
            legend=False)
axes[1,0].set_title('PETAL LENGTH DISTRIBUTION BY SPECIES', fontweight='bold')
axes[1,0].set_ylabel('Petal Length (cm)')
axes[1,0].set_xlabel('Specie')

# Sepal width distribution located in axes[1,1]
sns.histplot(data=df, x='sepal width (cm)', hue='species', 
             ax=axes[1,1], palette=['#E61313', '#0CCF60', '#170DDF'], 
             alpha=0.6, element='step')
axes[1,1].set_title('SEPAL WIDTH DISTRIBUTION BY SPECIES', fontweight='bold')
axes[1,1].set_xlabel('Sepal Width (cm)')

# Last step of creating subplots
plt.tight_layout()
plt.show()</code></pre>

<h2>Accuracy results</h2>
<pre><code>print("=" * 40)
print(f"Cross-validation accuracy: {svm_grid.best_score_:.3f}")
print(f"Test set accuracy: {accuracy_score(y_test, y_pred_svm):.3f}")
</code></pre>
Regarding accuracy results we have:
<strong>Cross-validation accuracy:</strong> 0.975
<strong>Test set accuracy:</strong> 0.967
<h2>Metrics by class</h2>
<pre><code>precision = precision_score(y_test, y_pred_svm, average=None)
recall = recall_score(y_test, y_pred_svm, average=None)
f1 = f1_score(y_test, y_pred_svm, average=None)

report = classification_report(y_test, y_pred_svm, target_names=target_names, digits=5)
print(f"{report}")
</code></pre>
The classification report:
              precision    recall  f1-score   support

      setosa    1.00000   1.00000   1.00000        10
  versicolor    1.00000   0.90000   0.94737        10
   virginica    0.90909   1.00000   0.95238        10

    accuracy                        0.96667        30
   macro avg    0.96970   0.96667   0.96658        30
weighted avg    0.96970   0.96667   0.96658        30

The empty cells in the "accuracy" row (under precision and recall) exist because accuracy is a single overall metric for the model, not broken down by those categories.<h2>Create dataframe with predictions</h2>
<pre><code>results_df = pd.DataFrame({
    'Real': y_test,
    'Predicción': y_pred_svm,
    'Correcto': y_test == y_pred_svm
})

results_df['Real_Especie'] = results_df['Real'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})
results_df['Pred_Especie'] = results_df['Predicción'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

print(f"NUMBER OF CORRECT PREDICTIONS: {results_df['Correcto'].sum()}/{len(results_df)}")
print(f"FINAL ACCURACY: {accuracy_score(y_test, y_pred_svm):.3f}")
print(f"BEST HYPERPARAMETERS: {svm_grid.best_params_}")
</code></pre>
Output:
NUMBER OF CORRECT PREDICTIONS: 29/30
FINAL ACCURACY: 0.967
BEST HYPERPARAMETERS: {'svm__C': 1, 'svm__gamma': 0.1}
			<h2>Conclusions</h2>
                <ul>
                    <li>The Iris dataset contains 3 balanced species (50 samples each)</li>
                    <li>Petal features are more discriminative than sepal features</li>
                    <li>The SVM model achieves high accuracy in multiclass classification</li>
                    <li>The species <em>setosa</em> is the easiest to classify, however, <em>versicolor</em> and <em>virginica</em> show some overlap</li>
                </ul>
<p>The SVM model is suitable for botanical classification systems and can be implemented in flower identification applications.</p>
					<a href="https://github.com/sergioiglesias1" class="button">View Full Code on GitHub</a>
				</div>
			</article>
		</div>
	</div>

	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
</body>

</html>


